{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yaml\n",
    "from onsager import OnsagerCalc\n",
    "from onsager import crystal\n",
    "from onsager import crystalStars as stars\n",
    "from scipy.constants import physical_constants\n",
    "kB = physical_constants['Boltzmann constant in eV/K'][0]\n",
    "def build_point_group_ops(lattice, basis, threshold=1e-8):\n",
    "\n",
    "        pgroup_ops = []\n",
    "        sgroup_ops = []\n",
    "        inv_lat_vec = np.linalg.inv(lattice)\n",
    "        supercellvect = [np.array((n0, n1, n2))\n",
    "                         for n0 in range(-1, 2)\n",
    "                         for n1 in range(-1, 2)\n",
    "                         for n2 in range(-1, 2)\n",
    "                         if (n0, n1, n2) != (0, 0, 0)]\n",
    "        nmat_list = [X for X in [np.array((n0, n1, n2))\n",
    "                                 for n0 in supercellvect\n",
    "                                 for n1 in supercellvect\n",
    "                                 for n2 in supercellvect] if abs(np.linalg.det(X)) == 1]\n",
    "        for nmat in nmat_list:\n",
    "            g = np.dot(lattice, np.dot(nmat, inv_lat_vec))\n",
    "            if np.all(abs(np.dot(g.T, g) - np.eye(3)) < threshold):\n",
    "                flag_op = 1\n",
    "                for bas in basis:\n",
    "                    vec1 = bas - basis[0]\n",
    "                    for i, j in enumerate(vec1):\n",
    "                        if j < 0:\n",
    "                            vec1[i] += 1.0\n",
    "                    vec2 = np.dot(nmat, vec1)\n",
    "                    for i, j in enumerate(vec2):\n",
    "                        if j < 0:\n",
    "                            vec2[i] += 1.0\n",
    "                    if np.any(abs(vec1 - vec2) > threshold):\n",
    "                        flag_op = 0\n",
    "                        sgroup_ops.append(nmat)\n",
    "\n",
    "                if flag_op:\n",
    "                    pgroup_ops.append(nmat)\n",
    "        return np.array(pgroup_ops), np.array(sgroup_ops) \n",
    "        \n",
    "\n",
    "def apply_pg_site_dir(pg, site1, site2, threshold=1e-8):\n",
    "        \n",
    "    if np.any([np.all(abs(site1 - np.dot(g, site2)) < threshold) for g in pg]):\n",
    "            return 1\n",
    "    else:\n",
    "            return 0\n",
    "\n",
    "        \n",
    "def apply_sg_site_dir(sg, site1, site2, threshold=1e-8):\n",
    "        \n",
    "    if np.any([np.all(abs(site1 - np.dot(g, site2)) < threshold) for g in sg]):\n",
    "            return 1\n",
    "    else:\n",
    "            return 0\n",
    "\n",
    "def apply_pg_trans(pg, trans1, trans2, threshold=1e-8):\n",
    "\n",
    "    if np.any([np.all(abs(trans1[0] - np.dot(g, trans2[0])) < threshold) and np.all(abs(trans1[1] - np.dot(g, trans2[1])) < threshold) for g in pg]):\n",
    "            return 1\n",
    "    else:\n",
    "            return 0\n",
    "\n",
    "        \n",
    "def apply_sg_trans(sg, trans1, trans2, threshold=1e-8):\n",
    "        \n",
    "    if np.any([np.all(abs(trans1[0] - np.dot(g, trans2[0])) < threshold)and np.all(abs(trans1[1] - np.dot(g, trans2[1])) < threshold) for g in sg]):\n",
    "            return 1\n",
    "    else:\n",
    "            return 0\n",
    "def read_data(data_file):\n",
    "    site_data = []\n",
    "    trans_data = []\n",
    "    dict_loader = yaml.load(data_file)\n",
    "    vacancy_data = dict_loader.get('vacancy_data')  # vacancy data  \n",
    "    site_data = dict_loader.get('site_data')  # site data\n",
    "    trans_data = dict_loader.get('trans_data')  # transition data\n",
    "    for i, data in enumerate(trans_data):\n",
    "        if len(data) == 6:\n",
    "            trans_data[i].append([0, 0.0, 0.0])\n",
    "    bulk = dict_loader.get('bulk')      \n",
    "    for i in range(len(site_data)):\n",
    "        site_data[i][2]=0.0\n",
    "        site_data[i][1]=1.0\n",
    "    for i in range(len(trans_data)):\n",
    "        trans_data[i][3]=0.0\n",
    "        trans_data[i][5]=0.0\n",
    "        if trans_data[i][6][0]:\n",
    "            trans_data[i][6][1]=1e-6\n",
    "            trans_data[i][6][2]=0.0\n",
    "            trans_data[i][2]=1.0\n",
    "            trans_data[i][4]=1.0\n",
    "        else:\n",
    "            trans_data[i][2]=0.5\n",
    "            trans_data[i][4]=0.5\n",
    "            \n",
    "    w0_data = []\n",
    "    w1_data = []\n",
    "    w2_data = []\n",
    "    for data in trans_data:\n",
    "        if np.all(data[0]==[0,0,0]):\n",
    "            w0_data.append(data)\n",
    "        elif np.all(data[1]==[0,0,0]):\n",
    "            w2_data.append(data)\n",
    "        else:\n",
    "            w1_data.append(data)\n",
    "    return vacancy_data, site_data, w0_data, w1_data, w2_data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## get data\n",
    "data = open('Sn_data_temp.yaml', 'r')\n",
    "vacancy_data, site_data, w0_data, w1_data, w2_data   = read_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## get list of deleted states and meta states\n",
    "#for state in starset.states:\n",
    "#    if state.i not in meta_sites and state.j in meta_sites:\n",
    "#        if np.allclose(0.866025403784,np.linalg.norm(state.dx)):\n",
    "#            to_del.append(state)\n",
    "deleted_states = []\n",
    "meta_states = []\n",
    "new_w1_data = []\n",
    "for jump in w1_data:\n",
    "    if (jump[-1][0]):\n",
    "        meta = np.array((np.array(jump[0])+np.array(jump[1]))/2).tolist()\n",
    "        meta_states.append([meta,1e-6,0.0])\n",
    "        new_jump1 = [jump[0],meta,jump[2],jump[3],jump[2],jump[3]]\n",
    "        new_jump2 = [jump[1],meta,jump[4],jump[5],jump[4],jump[5]]\n",
    "        new_w1_data.append(new_jump1)\n",
    "        new_w1_data.append(new_jump2)\n",
    "    else:\n",
    "        deleted_states.append((np.array(jump[0])+np.array(jump[1]))/2)\n",
    "        new_w1_data.append(jump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deleted_states.append(np.array([0.5,0.5,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Lattice:\n",
      "  a1 = [ 0.5       -0.8660254  0.       ]\n",
      "  a2 = [ 0.5        0.8660254  0.       ]\n",
      "  a3 = [ 0.          0.          1.63299316]\n",
      "#Basis:\n",
      "  (Zr) 0.0 = [ 0.33333333  0.66666667  0.25      ]\n",
      "  (Zr) 0.1 = [ 0.66666667  0.33333333  0.75      ]\n"
     ]
    }
   ],
   "source": [
    "a= 1.0#3.2342373809\n",
    "c_a= np.sqrt(8/3) #1.5989108537\n",
    "c=a*c_a\n",
    "HCP = crystal.Crystal.HCP(a0=a,c_a=c_a, chemistry=\"Zr\")\n",
    "pg,sg = build_point_group_ops(HCP.lattice/a, HCP.basis[0])\n",
    "len(pg)\n",
    "print(HCP)\n",
    "meta_basis = HCP.Wyckoffpos(np.array([5/6,2/3,0.25]))\n",
    "basis = HCP.basis[0] + meta_basis\n",
    "HCPmeta = crystal.Crystal(HCP.lattice, basis[0:8], chemistry=[\"Zr\"], noreduce=True)\n",
    "sitelist = HCPmeta.sitelist(0)\n",
    "vacancyjumps = HCPmeta.jumpnetwork(0, 1.01*a)\n",
    "meta_sites = np.arange(2,8,1)\n",
    "for pos,jlist in enumerate(vacancyjumps):\n",
    "        if np.any([np.allclose(dx,a*np.array([0.5, -0.8660254, 0.])) for (i,j), dx in jlist]):\n",
    "            ind1 = pos\n",
    "            break\n",
    "#print(\"ind1 = \",ind1)\n",
    "for pos,jlist in enumerate(vacancyjumps):\n",
    "        if np.any([np.allclose(dx,a*np.array([ 0.25, -0.4330127, 0.])) for (i,j), dx in jlist]):\n",
    "            ind2 = pos\n",
    "            break\n",
    "#print(\"ind2 = \",ind2)\n",
    "jumpnetwork = [vacancyjumps[1], vacancyjumps[ind2]]\n",
    "jumpnetwork2 = [vacancyjumps[1], vacancyjumps[ind1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Lattice:\n",
      "  a1 = [ 0.5       -0.8660254  0.       ]\n",
      "  a2 = [ 0.5        0.8660254  0.       ]\n",
      "  a3 = [ 0.          0.          1.63299316]\n",
      "#Basis:\n",
      "  (Zr) 0.0 = [ 0.33333333  0.66666667  0.25      ]\n",
      "  (Zr) 0.1 = [ 0.66666667  0.33333333  0.75      ]\n",
      "  (Zr) 0.2 = [ 0.16666667  0.33333333  0.75      ]\n",
      "  (Zr) 0.3 = [ 0.33333333  0.16666667  0.25      ]\n",
      "  (Zr) 0.4 = [ 0.16666667  0.83333333  0.75      ]\n",
      "  (Zr) 0.5 = [ 0.83333333  0.66666667  0.25      ]\n",
      "  (Zr) 0.6 = [ 0.66666667  0.83333333  0.75      ]\n",
      "  (Zr) 0.7 = [ 0.83333333  0.16666667  0.25      ]\n"
     ]
    }
   ],
   "source": [
    "print(HCPmeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "starset = stars.StarSetMeta(jumpnetwork, HCPmeta, 0, meta_sites = meta_sites)\n",
    "starset.generate(2)\n",
    "to_del = []\n",
    "for i, state in enumerate(starset.states):\n",
    "    if state.i not in meta_sites and state.j in meta_sites:\n",
    "        if state.i==0: \n",
    "            if np.any([apply_pg_site_dir(pg,np.dot(HCP.invlatt,state.dx), site) for site in deleted_states]):\n",
    "                to_del.append(state)\n",
    "                #print(i)\n",
    "        elif state.i==1: \n",
    "            if np.any([apply_sg_site_dir(sg,np.dot(HCP.invlatt,state.dx), site) for site in deleted_states]):\n",
    "                to_del.append(state)                \n",
    "                #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HCPdiffuser = OnsagerCalc.VacancyMediatedMeta(HCPmeta, 0, sitelist, jumpnetwork, 2, meta_sites = np.arange(2,8,1), jumpnetwork2= jumpnetwork2, deleted_states=to_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binding_entropy_list = []\n",
    "binding_energy_list = []\n",
    "for i,state in enumerate(HCPdiffuser.interactlist()):\n",
    "        data_not_found = 1\n",
    "        #print(i,state)\n",
    "        for site in site_data:\n",
    "            if state.i==0: \n",
    "                if apply_pg_site_dir(pg,np.dot(HCP.invlatt,state.dx), site[0]):\n",
    "                    #print(site[0],site[2])\n",
    "                    binding_entropy_list.append(site[1])\n",
    "                    binding_energy_list.append(site[2])\n",
    "                    data_not_found = 0\n",
    "                    break\n",
    "            else:        \n",
    "                if apply_sg_site_dir(sg,np.dot(HCP.invlatt,state.dx), site[0]):\n",
    "                    #print(site[0],site[2])\n",
    "                    binding_entropy_list.append(site[1])\n",
    "                    binding_energy_list.append(site[2])\n",
    "                    data_not_found = 0\n",
    "                    break\n",
    "        if data_not_found:\n",
    "            for site in meta_states:\n",
    "                if state.i==0: \n",
    "                    if apply_pg_site_dir(pg,np.dot(HCP.invlatt,state.dx), site[0]):\n",
    "                        #print(site[0],site[2])\n",
    "                        binding_entropy_list.append(site[1])\n",
    "                        binding_energy_list.append(0.0)\n",
    "                        data_not_found = 0\n",
    "                        break\n",
    "                else:        \n",
    "                    if apply_sg_site_dir(sg,np.dot(HCP.invlatt,state.dx), site[0]):\n",
    "                        #print(site[0],site[2])\n",
    "                        binding_entropy_list.append(site[1])\n",
    "                        binding_energy_list.append(0.0)\n",
    "                        data_not_found = 0\n",
    "                        break\n",
    "        if data_not_found:\n",
    "            if state.i in meta_sites or state.j in meta_sites:\n",
    "                    #print(\"no data; setting binding energy = 0\")\n",
    "                    binding_entropy_list.append(1.0)\n",
    "                    binding_energy_list.append(0.0)\n",
    "            else:\n",
    "                    #print(\"no data; setting binding energy = 0\")\n",
    "                    binding_entropy_list.append(1.0)\n",
    "                    binding_energy_list.append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##HCPtracer = {'preV': np.array([1.0,1.0/10000000.0]), 'eneV': np.array([0.0, 0.0]),\n",
    "##             'preT0': np.array([ 1.0, 1.0]),\n",
    "##             'eneT0': np.array([0, 0]),\n",
    "##              }\n",
    "\n",
    "# HCPtracer = {'preV': np.array([1.0,54.169024409/18.299152044],), 'eneV': np.array([0.0,0.51727]),\n",
    "#              'preT0': np.array([ 54.169024409/9.26073917, 54.169024409/10.40702378]),\n",
    "#              'eneT0': np.array([0.613339999999994,0.553549999999973]),\n",
    "#               }\n",
    "\n",
    "HCPsolute = {'preV': np.array([1.0,1e-6],), 'eneV': np.array([0.0,0.0]),\n",
    "             'preT0': np.array([ 0.5, 1.0]),\n",
    "             'eneT0': np.array([0.0,0.0]),\n",
    "              }\n",
    "HCPsolute['preSV'] = np.array(binding_entropy_list)\n",
    "HCPsolute['eneSV'] = np.array(binding_energy_list)\n",
    "HCPsolute['preS']= np.array([1.0,1.0])\n",
    "HCPsolute['eneS']= np.array([0.,0])\n",
    "# HCPtracer['preT2'] = np.array([ 54.169024409/9.26073917, 0.5* 54.169024409/10.40702378,54.169024409/10.40702378])\n",
    "# HCPtracer['eneT2'] = np.array([0.613339999999994,0.553549999999973,1e12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eneT2 [ 0.  0.]\n",
      "eneV [ 0.  0.]\n",
      "preT2 [ 0.5  1. ]\n",
      "eneT1 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "preT0 [ 0.5  1. ]\n",
      "preSV [ 1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "preT1 [ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  1.   1.   1.   1.\n",
      "  1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1. ]\n",
      "preV [  1.00000000e+00   1.00000000e-06]\n",
      "eneT0 [ 0.  0.]\n",
      "preS [ 1.  1.]\n",
      "eneS [ 0.  0.]\n",
      "eneSV [ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "HCPsolute.update(HCPdiffuser.makeLIMBpreene(**HCPsolute))\n",
    "for k,v in zip(HCPsolute.keys(), HCPsolute.values()): print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "templist = []\n",
    "for i,j in enumerate(HCPsolute['preT1']):\n",
    "    if j==0.001:\n",
    "        templist.append(i)\n",
    "        HCPsolute['preT1'][i]=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [-0.5        -0.28867513  0.81649658] [ 0.5         0.28867513 -0.81649658]\n",
      "[0.33333333, 0.66666667, -0.5] [0.0, 0.0, 0.0] 0.5 0.0\n",
      "0 [-0.5       -0.8660254  0.       ] [ 0.5        0.8660254  0.       ]\n",
      "[1.0, 1.0, 0.0] [0.0, 0.0, 0.0] 0.5 0.0\n"
     ]
    }
   ],
   "source": [
    "omega2=HCPdiffuser.omegalist(2)[0]\n",
    "for j, (S1,S2) in enumerate(omega2):\n",
    "    data_not_found = 1\n",
    "    print(S1.i,S1.dx,S2.dx)\n",
    "    if S1.i==0:\n",
    "        for trans in w2_data:                        \n",
    "            if apply_pg_site_dir(pg,np.dot(HCP.invlatt,S1.dx), trans[0]): \n",
    "                print(trans[0],trans[1],trans[2],trans[3])\n",
    "                HCPsolute['eneT2'][j]= trans[3] + HCPsolute['eneV'][0] + HCPsolute['eneS'][0] \n",
    "                HCPsolute['preT2'][j]= trans[2]\n",
    "                data_not_found = 0\n",
    "                break\n",
    "    else:\n",
    "        for trans in w2_data:                        \n",
    "            if apply_sg_site_dir(sg,np.dot(HCP.invlatt,S1.dx), trans[0]): \n",
    "                print(trans[0],trans[1],trans[2],trans[3])\n",
    "                HCPsolute['eneT2'][j]= trans[3] + HCPsolute['eneV'][0] + HCPsolute['eneS'][0] \n",
    "                HCPsolute['preT2'][j]= trans[2]\n",
    "                data_not_found = 0\n",
    "                break\n",
    "    if data_not_found:\n",
    "            print(\"no data; limb used\")\n",
    "            #binding_entropy_list.append(1)\n",
    "            #binding_energy_list.append(0.0)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 [0.33333333, 0.66666667, -0.5] [0.0, 0.0, -1.0] 0.5 0.0\n",
      "1 0 [0.33333333, 0.66666667, -0.5] [0.0, 1.0, -1.0] 0.5 0.0\n",
      "2 1 [0.33333333, 0.66666667, -0.5] [0.0, 1.0, 0.0] 0.5 0.0\n",
      "3 7 [1.0, 1.0, 0.0] [1.33333333, 1.66666667, -0.5] 0.5 0.0\n",
      "4 6 [1.0, 1.0, 0.0] [1.33333333, 0.66666667, -0.5] 0.5 0.0\n",
      "no data; limb used\n",
      "no data; limb used\n",
      "no data; limb used\n",
      "no data; limb used\n",
      "no data; limb used\n",
      "no data; limb used\n",
      "11 4 [0.33333333, 0.66666667, -0.5] [-0.66666667, 0.66666667, -0.5] 0.5 0.0\n",
      "12 5 [0.33333333, 0.66666667, -0.5] [0.33333333, -0.33333333, -0.5] 0.5 0.0\n",
      "13 11 [1.0, 1.0, 0.0] [1.0, 0.5, 0.0] 1.0 0.0\n",
      "no data; limb used\n",
      "no data; limb used\n",
      "no data; limb used\n",
      "17 3 [0.33333333, 0.66666667, -0.5] [0.33333333, 1.66666667, -0.5] 0.5 0.0\n",
      "18 8 [1.0, 1.0, 0.0] [0.5, 1.0, 0.0] 1.0 0.0\n",
      "no data; limb used\n",
      "no data; limb used\n",
      "no data; limb used\n",
      "no data; limb used\n",
      "no data; limb used\n",
      "no data; limb used\n",
      "no data; limb used\n"
     ]
    }
   ],
   "source": [
    "omega1=HCPdiffuser.omegalist(1)[0]\n",
    "for j, (S1,S2) in enumerate(omega1):\n",
    "    data_not_found = 1    \n",
    "    if S1.i==0:\n",
    "        for i,trans in enumerate(new_w1_data):                        \n",
    "            if apply_pg_trans(pg,np.array([np.dot(HCP.invlatt,S1.dx),np.dot(HCP.invlatt,S2.dx)]),np.array([trans[0],trans[1]])):\n",
    "                #print(S1.i,S1.dx,S2.dx)\n",
    "                print(j,i,trans[0],trans[1],trans[2],trans[3])\n",
    "                HCPsolute['eneT1'][j]= trans[3] + HCPsolute['eneV'][0] + HCPsolute['eneS'][0] \n",
    "                HCPsolute['preT1'][j]= trans[2]\n",
    "                data_not_found = 0\n",
    "                break\n",
    "            elif apply_pg_trans(pg,np.array([np.dot(HCP.invlatt,S1.dx),np.dot(HCP.invlatt,S2.dx)]),np.array([trans[1],trans[0]])):\n",
    "                #print(S1.i,S1.dx,S2.dx)\n",
    "                print(j,i,trans[0],trans[1],trans[2],trans[3])\n",
    "                HCPsolute['eneT1'][j]= trans[3] + HCPsolute['eneV'][0] + HCPsolute['eneS'][0] \n",
    "                HCPsolute['preT1'][j]= trans[2]\n",
    "                data_not_found = 0\n",
    "                break\n",
    "    elif S1.i==1:\n",
    "        for i,trans in enumerate(new_w1_data):                              \n",
    "            if apply_sg_trans(sg,np.array([np.dot(HCP.invlatt,S1.dx),np.dot(HCP.invlatt,S2.dx)]),np.array([trans[0],trans[1]])):\n",
    "                #print(S1.i,S1.dx,S2.dx)\n",
    "                print(j,i,trans[0],trans[1],trans[2],trans[3])\n",
    "                HCPsolute['eneT1'][j]= trans[3] + HCPsolute['eneV'][0] + HCPsolute['eneS'][0] \n",
    "                HCPsolute['preT1'][j]= trans[2]\n",
    "                data_not_found = 0\n",
    "                break\n",
    "            elif apply_sg_trans(sg,np.array([np.dot(HCP.invlatt,S1.dx),np.dot(HCP.invlatt,S2.dx)]),np.array([trans[1],trans[0]])):\n",
    "                #print(S1.i,S1.dx,S2.dx)\n",
    "                print(j,i,trans[0],trans[1],trans[2],trans[3])\n",
    "                HCPsolute['eneT1'][j]= trans[3] + HCPsolute['eneV'][0] + HCPsolute['eneS'][0] \n",
    "                HCPsolute['preT1'][j]= trans[2]\n",
    "                data_not_found = 0\n",
    "                break\n",
    "    if data_not_found:\n",
    "            print(\"no data; limb used\")\n",
    "            #if np.isclose(HCPsolute['eneT1'][j],0.55355):\n",
    "            #    HCPsolute['preT1'][j]=5.20504474229\n",
    "            #print(HCPsolute['eneT1'][j],HCPsolute['preT1'][j])\n",
    "            #binding_entropy_list.append(1)\n",
    "            #binding_energy_list.append(0.0)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eneT2 [ 0.  0.]\n",
      "eneV [ 0.  0.]\n",
      "preT2 [ 0.5  0.5]\n",
      "eneT1 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "preT0 [ 0.5  1. ]\n",
      "preSV [ 1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "preT1 [ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  1.   1.\n",
      "  1.   1.   0.5  1.   1.   1.   1.   1.   1.   1.   1. ]\n",
      "preV [  1.00000000e+00   1.00000000e-06]\n",
      "eneT0 [ 0.  0.]\n",
      "preS [ 1.  1.]\n",
      "eneS [ 0.  0.]\n",
      "eneSV [ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "for k,v in zip(HCPsolute.keys(), HCPsolute.values()): print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 -8.26174565923 -8.26174565923 -1.0\n",
      "1000 -10.5756447926 -10.5756447926 -1.27967007336\n",
      "1000 7.81202568366e-09 7.81202568366e-09 7.81449078808e-09\n",
      "1000 0.999997000009 0.999997000009 0.999997000009\n"
     ]
    }
   ],
   "source": [
    "#Temp=np.arange(500, 1010, 100)\n",
    "#Temp = [500,1000]\n",
    "Temp = [1000]\n",
    "D_Onsager=[]\n",
    "for T in Temp:\n",
    "    pre=1e-8 # THz and Angstrom unit scaling\n",
    "    Lvv, Lss, Lsv, L1vv = HCPdiffuser.Lij(*HCPdiffuser.preene2betafree(kB*T, **HCPsolute))\n",
    "    #Lss=Lss*pre\n",
    "    #D_Onsager.append([Lss[0,0],Lss[1,1],Lss[2,2]])        \n",
    "    print(T, Lsv[0,0]/Lvv[0,0],Lsv[1,1]/Lvv[1,1],Lsv[2,2]/Lvv[2,2])\n",
    "    print(T, Lsv[0,0]/Lss[0,0],Lsv[1,1]/Lss[1,1],Lsv[2,2]/Lss[2,2])\n",
    "    Lss=Lss*pre\n",
    "    Lvv=Lvv\n",
    "    print(T, Lss[0,0],Lss[1,1],Lss[2,2])    \n",
    "    print(T, Lvv[0,0],Lvv[1,1],Lvv[2,2])\n",
    "    \n",
    "D_Onsager=np.array(D_Onsager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ -2.88675135e-01,   6.40987562e-17,   0.00000000e+00]),\n",
       " array([ 0.14433757, -0.25      ,  0.        ]),\n",
       " array([ 0.14433757,  0.25      ,  0.        ]),\n",
       " array([ -2.88675135e-01,  -6.40987562e-17,   0.00000000e+00]),\n",
       " array([ 0.14433757,  0.25      ,  0.        ]),\n",
       " array([-0.14433757,  0.25      ,  0.        ]),\n",
       " array([ 0.28867513,  0.        ,  0.        ]),\n",
       " array([-0.14433757, -0.25      ,  0.        ]),\n",
       " array([ 0.28867513,  0.        ,  0.        ]),\n",
       " array([ 0.14433757, -0.25      ,  0.        ]),\n",
       " array([-0.14433757,  0.25      ,  0.        ]),\n",
       " array([-0.14433757, -0.25      ,  0.        ])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HCPdiffuser.vkinetic.vecvec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  2.88675135e-01,   6.40987562e-17,  -0.00000000e+00]),\n",
       " array([  2.88675135e-01,  -6.40987562e-17,  -0.00000000e+00]),\n",
       " array([-0.14433757,  0.25      , -0.        ]),\n",
       " array([-0.14433757, -0.25      , -0.        ]),\n",
       " array([-0.14433757, -0.25      , -0.        ]),\n",
       " array([ 0.14433757, -0.25      , -0.        ]),\n",
       " array([-0.28867513, -0.        , -0.        ]),\n",
       " array([ 0.14433757,  0.25      , -0.        ]),\n",
       " array([-0.28867513, -0.        , -0.        ]),\n",
       " array([-0.14433757,  0.25      , -0.        ]),\n",
       " array([ 0.14433757, -0.25      , -0.        ]),\n",
       " array([ 0.14433757,  0.25      , -0.        ])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HCPdiffuser.vkinetic.vecvec[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.        , -0.16666667,  0.23570226]),\n",
       " array([-0.14433757, -0.08333333, -0.23570226]),\n",
       " array([ 0.        ,  0.16666667, -0.23570226]),\n",
       " array([-0.14433757,  0.08333333,  0.23570226]),\n",
       " array([ 0.14433757, -0.08333333, -0.23570226]),\n",
       " array([ 0.14433757,  0.08333333,  0.23570226]),\n",
       " array([-0.14433757,  0.08333333, -0.23570226]),\n",
       " array([ 0.14433757, -0.08333333,  0.23570226]),\n",
       " array([-0.        , -0.16666667, -0.23570226]),\n",
       " array([ 0.14433757,  0.08333333, -0.23570226]),\n",
       " array([-0.14433757, -0.08333333,  0.23570226]),\n",
       " array([ 0.        ,  0.16666667,  0.23570226])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HCPdiffuser.vkinetic.vecvec[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 10, 12, 14, 18, 19, 20, 24, 25, 26, 30, 31]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HCPdiffuser.vkinetic.vecpos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 11, 13, 15, 16, 17, 21, 22, 23, 27, 28, 29]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HCPdiffuser.vkinetic.vecpos[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56, 57, 58, 59, 61, 62, 64, 71, 74, 75, 78, 79]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HCPdiffuser.vkinetic.vecpos[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5],\n",
       " [6, 7],\n",
       " [8, 10, 12, 14, 18, 19, 20, 24, 25, 26, 30, 31],\n",
       " [9, 11, 13, 15, 16, 17, 21, 22, 23, 27, 28, 29],\n",
       " [32, 43, 46, 52, 54, 55],\n",
       " [33, 34, 35, 36, 40, 44, 45, 47, 49, 50, 51, 53],\n",
       " [37, 38, 39, 41, 42, 48],\n",
       " [56, 57, 58, 59, 61, 62, 64, 71, 74, 75, 78, 79],\n",
       " [60, 63, 65, 66, 67, 68, 69, 70, 72, 73, 76, 77],\n",
       " [80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103],\n",
       " [104,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  112,\n",
       "  114,\n",
       "  115,\n",
       "  117,\n",
       "  119,\n",
       "  121,\n",
       "  122,\n",
       "  124,\n",
       "  126,\n",
       "  131,\n",
       "  132,\n",
       "  134,\n",
       "  135,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  148,\n",
       "  149],\n",
       " [105, 111, 118, 123, 125, 130, 133, 138, 142, 144, 146, 147],\n",
       " [113, 116, 120, 127, 128, 129, 136, 137, 143, 145, 150, 151],\n",
       " [152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163],\n",
       " [164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175],\n",
       " [176, 177, 178, 179],\n",
       " [180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  195,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  203],\n",
       " [204,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  208,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  225,\n",
       "  226,\n",
       "  227],\n",
       " [228, 229, 233, 236, 238, 239, 241, 244, 246, 247, 250, 251],\n",
       " [230, 231, 232, 234, 235, 237, 240, 242, 243, 245, 248, 249],\n",
       " [252,\n",
       "  253,\n",
       "  254,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  265,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  272,\n",
       "  273,\n",
       "  274,\n",
       "  275],\n",
       " [276,\n",
       "  277,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  284,\n",
       "  287,\n",
       "  288,\n",
       "  291,\n",
       "  293,\n",
       "  296,\n",
       "  298,\n",
       "  301,\n",
       "  302,\n",
       "  305,\n",
       "  308,\n",
       "  309,\n",
       "  310,\n",
       "  312,\n",
       "  313,\n",
       "  314,\n",
       "  318,\n",
       "  319,\n",
       "  323],\n",
       " [278,\n",
       "  282,\n",
       "  283,\n",
       "  285,\n",
       "  286,\n",
       "  289,\n",
       "  290,\n",
       "  292,\n",
       "  294,\n",
       "  295,\n",
       "  297,\n",
       "  299,\n",
       "  300,\n",
       "  303,\n",
       "  304,\n",
       "  306,\n",
       "  307,\n",
       "  311,\n",
       "  315,\n",
       "  316,\n",
       "  317,\n",
       "  320,\n",
       "  321,\n",
       "  322],\n",
       " [324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  338,\n",
       "  339,\n",
       "  340,\n",
       "  341,\n",
       "  342,\n",
       "  343,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  347],\n",
       " [348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359],\n",
       " [360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371],\n",
       " [372,\n",
       "  373,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  377,\n",
       "  378,\n",
       "  379,\n",
       "  380,\n",
       "  381,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  385,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  390,\n",
       "  391,\n",
       "  392,\n",
       "  393,\n",
       "  394,\n",
       "  395]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HCPdiffuser.vkinetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
